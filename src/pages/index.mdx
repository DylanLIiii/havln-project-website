---
layout: ../layouts/Layout.astro
title: Human-Aware Vison-and-Language Navigation
description: Project page of paper "Human-Aware Vison-and-Language Navigation"
favicon: favicon.svg
thumbnail: screenshot.png
---

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import outside from "../assets/outside.mp4";
import transformer from "../assets/transformer.webp";
import FigScenario from "../assets/task_define_final/task_define_final-1.png";
import Splat from "../components/Splat.tsx"

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  {/* authors={[
    {
      name: "Roman Hauksson",
      url: "https://roman.technology",
      institution: "Independent",
      notes: ["*", "†"],
    },
    {
      name: "Author Two",
      institution: "Institution Two",
      notes: ["*", "†"],
    },
    {
      name: "Author Three",
      institution: "Institution Three",
      notes: ["†"],
    },
    {
      name: "Author Four",
      institution: "Institution Four",
    },
  ]} */}
  conference="Under Review"
  {/* notes={[
    {
      symbol: "*",
      text: "author note one",
    },
    {
      symbol: "†",
      text: "author note two",
    },
  ]} */}
  links={[
    {
      name: "Paper",
      url: "",
      icon: "fa-solid:file-pdf",
    },
    {
      name: "Code",
      url: "",
      icon: "mdi:github",
    },
    {
      name: "arXiv",
      url: "",
      icon: "academicons:arxiv",
    },
  ]}
  />

<Video source={outside} />

<HighlightedSection>

## Abstract

Vision-and-Language Navigation (VLN) is crucial for enabling robots to assist humans in everyday environments. However, current VLN systems lack social awareness and rely on simplified instructions with static environments, limiting Sim2Real realizations. To narrow these gaps, we present Human-Aware Vision-and-Language Navigation (HA-VLN), expanding VLN to include both discrete (HA-VLN-DE) and continuous (HA-VLN-CE) environments with social behaviors. The HA-VLN Simulator enables real-time rendering of human activities and provides unified environments for navigation development. It introduces the Human Activity and Pose Simulation (HAPS) Dataset 2.0 with detailed 3D human motion models and the HA Room-to-Room (HA-R2R) Dataset with complex navigation instructions that include human activities. We propose an HA-VLN Vision-and-Language model (HA-VLN-VL) and a Cross-Model Attention model (HA-VLN-CMA) to address visual-language understanding and dynamic decision-making challenges. Comprehensive evaluations and analysis show that dynamic environments with human activities significantly challenge current systems, highlighting the need for specialized human-aware navigation systems for real-world deployment.

</HighlightedSection>

## Figures

figures

<Figure
    caption="**HA-VLN Navigation Scenario:** An agent navigates environments with dynamic human activities, avoiding collisions by adjusting its path based on instructions and observations. Agent positions (e.g., ①, ②) align with *instructions* related to human movements. Agent decisions (e.g., **A**, **B**) represent actions taken in response to observed activities. For instance, at position ②, Decision **A**, the agent encounters a person on the phone and turns right to avoid collision. **R** **G** **B** and **Depth** observations (right side) show agent views preceding Decisions **A**, **B**, and **C**, capturing the agent's dynamic responses to human actions. *(Zoom in for more details.)*"
  >
    <Image source={FigScenario} altText="" />
</Figure>

## Two columns

maybe need two columns?

<TwoColumns>
  <Figure slot="left" caption="Take a look at this YouTube video.">
    <YouTubeVideo videoId="wjZofJX0v4M" />
  </Figure>
  <Figure slot="right" caption="Now look at this Gaussian Splat, rendered with a React component.">
    <Splat client:idle />
  </Figure>
</TwoColumns>

## Validation on Real-world Robots

## LaTeX

You can also add LaTeX formulas, rendered during the build process using [KaTeX](https://katex.org/) so they're quick to load for visitors of your project page. You can write them inline, like this: <LaTeX inline formula="a^2 + b^2 = c^2" />. Or, you can write them as a block:

<LaTeX formula="\int_a^b f(x) dx" />

## Tables

You can add simple tables using [GitHub Flavored Markdown syntax](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/organizing-information-with-tables):

| Model | Accuracy | F1 score | Training time (hours) |
| :--- | :---: | :---: | :---: |
| BERT-base | 0.89 | 0.87 | 4.5 |
| RoBERTa-large | 0.92 | 0.91 | 7.2 |
| DistilBERT | 0.86 | 0.84 | 2.1 |
| XLNet | 0.90 | 0.89 | 6.8 |

## BibTeX citation

```bibtex

```