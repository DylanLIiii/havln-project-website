---
layout: ../layouts/Layout.astro
title: Human-Aware Vison-and-Language Navigation
description: Project page of paper "Human-Aware Vison-and-Language Navigation"
favicon: favicon.svg
thumbnail: screenshot.png
---

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import outside from "../assets/outside.mp4";
import vidfront from "../assets/havln.mp4";
import visual from "../assets/visualization.mp4";
import transformer from "../assets/transformer.webp";
import FigScenario from "../assets/task_define_final/task_define_final-1.png";
import FigSim from "../assets/simulator_draft_v2/simulator_draft_v2-1.png";
import FigOverview from "../assets/overview_example-1.png";
import Splat from "../components/Splat.tsx"

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Anonymous",
    },
  ]}
  conference="Under Review"
  notes={[
    {
      symbol: "",
      text: "",
    },
  ]}
  links={[
    {
      name: "",
      url: "",
      icon: "",
    },
  ]}
  />

HA-VLN Navigation Scenario

<Figure
    caption="**HA-VLN Navigation Scenario:** An agent navigates environments with dynamic human activities, avoiding collisions by adjusting its path based on instructions and observations. Agent positions (e.g., ①, ②) align with *instructions* related to human movements. Agent decisions (e.g., **A**, **B**) represent actions taken in response to observed activities. For instance, at position ②, Decision **A**, the agent encounters a person on the phone and turns right to avoid collision. **R** **G** **B** and **Depth** observations (right side) show agent views preceding Decisions **A**, **B**, and **C**, capturing the agent's dynamic responses to human actions. *(Zoom in for more details.)*"
  >
    <Image source={FigScenario} altText="" />
</Figure>

<Video source={vidfront} />

<Figure
    caption="We present several annotated instances of human subjects from the proposed HAPS 2.0 Dataset, showcasing a variety of well-aligned motions, movements, and interactions"
  >
    <Image source={FigOverview} altText="" />
</Figure>

<HighlightedSection>

## Abstract

Vision-and-Language Navigation (VLN) is crucial for enabling robots to assist humans in everyday environments. However, current VLN systems lack social awareness and rely on simplified instructions with static environments, limiting Sim2Real realizations. To narrow these gaps, we present Human-Aware Vision-and-Language Navigation (HA-VLN), expanding VLN to include both discrete (HA-VLN-DE) and continuous (HA-VLN-CE) environments with social behaviors. The HA-VLN Simulator enables real-time rendering of human activities and provides unified environments for navigation development. It introduces the Human Activity and Pose Simulation (HAPS) Dataset 2.0 with detailed 3D human motion models and the HA Room-to-Room (HA-R2R) Dataset with complex navigation instructions that include human activities. We propose an HA-VLN Vision-and-Language model (HA-VLN-VL) and a Cross-Model Attention model (HA-VLN-CMA) to address visual-language understanding and dynamic decision-making challenges. Comprehensive evaluations and analysis show that dynamic environments with human activities significantly challenge current systems, highlighting the need for specialized human-aware navigation systems for real-world deployment.

</HighlightedSection>

## Figures

HA-VLN-CE Simulator

<Figure
    caption="HA-VLN-CE simulator incorporates dynamic human activities into photorealistic Habitat environments. The annotation process includes: 1). integrating the HAPS 2.0 dataset with 172 activities and 486 detailed 3D motion models across 58,320 frames; 2). a two-stage annotation—Stage 1: coarse-to-fine using PSO algorithm and multi-view cameras, and Stage 2: human-in-the-loop for enhancing multi-human interactions and movements; 3). real-time rendering using a signaling mechanism; and 4). enabling agent-environment interactions."
  >
    <Image source={FigSim} altText="" />
</Figure>

## Visualization


<Figure  
    caption="Visualization results of agent's trajectory"
  >
    <Video source={visual} altText="" />
</Figure>

## Dataset

[Download Here!](https://www.dropbox.com/scl/fo/6ofhh9vw5h21is38ahhgc/AOutW4EK3higqNOrX2hQ8rk?rlkey=v88np78ugr49z3sqisnvo6a9i&st=xogu3trq&dl=0)

## Validation on Real-world Robots

## BibTeX citation

```bibtex

```