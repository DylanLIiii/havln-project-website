---
layout: ../layouts/Layout.astro
title: Human-Aware Vison-and-Language Navigation
description: Project page of paper "Human-Aware Vison-and-Language Navigation"
favicon: favicon.svg
thumbnail: screenshot.png
---

import Layout from "../layouts/Layout.astro";

import Header from "../components/Header.astro";
import Video from "../components/Video.astro";
import HighlightedSection from "../components/HighlightedSection.astro";
import SmallCaps from "../components/SmallCaps.astro";
import Figure from "../components/Figure.astro";
import Image from "../components/Image.astro";
import TwoColumns from "../components/TwoColumns.astro";
import YouTubeVideo from "../components/YouTubeVideo.astro";
import LaTeX from "../components/LaTeX.astro";

import outside from "../assets/outside.mp4";
import vidfront from "../assets/8194nk5LbLH_all_ids_video.mp4";
import visual from "../assets/visualization.mp4";
import transformer from "../assets/transformer.webp";
import FigScenario from "../assets/task_define_final/task_define_final-1.png";
import Splat from "../components/Splat.tsx"

import CodeBlock from "../components/CodeBlock.astro";
import Table from "../components/Table.astro";
export const components = {pre: CodeBlock, table: Table}

<Header
  title={frontmatter.title}
  authors={[
    {
      name: "Anonymous",
    },
  ]}
  conference="Under Review"
  notes={[
    {
      symbol: "",
      text: "",
    },
  ]}
  links={[
    {
      name: "",
      url: "",
      icon: "",
    },
  ]}
  />

<Video source={vidfront} />

<HighlightedSection>

## Abstract

Vision-and-Language Navigation (VLN) is crucial for enabling robots to assist humans in everyday environments. However, current VLN systems lack social awareness and rely on simplified instructions with static environments, limiting Sim2Real realizations. To narrow these gaps, we present Human-Aware Vision-and-Language Navigation (HA-VLN), expanding VLN to include both discrete (HA-VLN-DE) and continuous (HA-VLN-CE) environments with social behaviors. The HA-VLN Simulator enables real-time rendering of human activities and provides unified environments for navigation development. It introduces the Human Activity and Pose Simulation (HAPS) Dataset 2.0 with detailed 3D human motion models and the HA Room-to-Room (HA-R2R) Dataset with complex navigation instructions that include human activities. We propose an HA-VLN Vision-and-Language model (HA-VLN-VL) and a Cross-Model Attention model (HA-VLN-CMA) to address visual-language understanding and dynamic decision-making challenges. Comprehensive evaluations and analysis show that dynamic environments with human activities significantly challenge current systems, highlighting the need for specialized human-aware navigation systems for real-world deployment.

</HighlightedSection>

## Figures

HA-VLN Navigation Scenario

<Figure
    caption="**HA-VLN Navigation Scenario:** An agent navigates environments with dynamic human activities, avoiding collisions by adjusting its path based on instructions and observations. Agent positions (e.g., ①, ②) align with *instructions* related to human movements. Agent decisions (e.g., **A**, **B**) represent actions taken in response to observed activities. For instance, at position ②, Decision **A**, the agent encounters a person on the phone and turns right to avoid collision. **R** **G** **B** and **Depth** observations (right side) show agent views preceding Decisions **A**, **B**, and **C**, capturing the agent's dynamic responses to human actions. *(Zoom in for more details.)*"
  >
    <Image source={FigScenario} altText="" />
</Figure>

## Visualization


<Figure  
    caption="Visualization results of agent's trajectory"
  >
    <Video source={visual} altText="" />
</Figure>

## Dataset

[Download Here!](https://www.dropbox.com/scl/fo/6ofhh9vw5h21is38ahhgc/AOutW4EK3higqNOrX2hQ8rk?rlkey=v88np78ugr49z3sqisnvo6a9i&st=xogu3trq&dl=0)

## Validation on Real-world Robots

## BibTeX citation

```bibtex

```